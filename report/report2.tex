\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=3.0cm]{geometry}
\usepackage{minted}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage[table]{xcolor}
\newcommand{\wo}{\mathbbm{w}}

\pdfinfo{
/Title (report2)
/Author (Felipe Salvatore)}
\setcounter{secnumdepth}{0}  


\title{Report: assigment 2}
\author{Felipe Salvatore\\
\texttt{felipessalvador@googlemail.com}}
\begin{document}
\maketitle
\section{1}
\textbf{1a)} 
\begin{minted}{python}
def softmax(x):
    # ## YOUR CODE HERE
    all_constants = - tf.reduce_max(x, axis=1)
    x = x + tf.expand_dims(all_constants, 1)
    x = tf.exp(x)
    all_sums = tf.reduce_sum(x, 1)
    all_sums = tf.pow(all_sums, -1)
    out = x*tf.expand_dims(all_sums, 1)
    # ## END YOUR CODE
    return out
\end{minted}


\textbf{1b)}
\begin{minted}{python}
def cross_entropy_loss(y, yhat):
    # ## YOUR CODE HERE
    y = tf.cast(y, tf.float32)
    yhat = tf.log(yhat)
    out = - tf.reduce_sum(y*yhat)
    out = tf.reshape(out, (1,))
    # ## END YOUR CODE
    return out
\end{minted}

\textbf{1c)} The placeholders variables are like their name suggest a placeholder for a tensor. We use it to form a computational graph before the training. In the training stage we use the dictionary feed\_dict to 'load' the placeholders variables with real tensors.

\begin{minted}{python}
    def add_placeholders(self):
        # ## YOUR CODE HERE
        self.input_placeholder = tf.placeholder(tf.float32,
                                                shape=[self.config.batch_size,
                                                       self.config.n_features],
                                                name="input_placeholder")
        self.labels_placeholder = tf.placeholder(tf.int32,
                                                 shape=[self.config.batch_size,
                                                        self.config.n_classes],
                                                 name="labels_placeholder")
        # ## END YOUR CODE

    def create_feed_dict(self, input_batch, label_batch):
        # ## YOUR CODE HERE
        feed_dict = {self.input_placeholder: input_batch,
                     self.labels_placeholder: label_batch}
        # ## END YOUR CODE
        return feed_dict
\end{minted}

\textbf{1d)}
\begin{minted}{python}
    def add_model(self, input_data):
        Wshape = [self.config.n_features, self.config.n_classes]
        bshape = [self.config.batch_size, self.config.n_classes]
        Winit = tf.zeros(Wshape)
        binit = tf.zeros(bshape)

        with tf.variable_scope("linear-model"):
            W = tf.get_variable("weights", dtype='float32', initializer=Winit)
            b = tf.get_variable("bias", dtype='float32', initializer=binit)
            out = softmax(tf.matmul(input_data, W) + b)
        # ## END YOUR CODE
        return out

    def add_loss_op(self, pred):
        # ## YOUR CODE HERE
        loss = cross_entropy_loss(self.labels_placeholder,
                                  pred)
        # ## END YOUR CODE
        return loss
\end{minted}


\textbf{1e)}
\begin{minted}{python}
    def add_training_op(self, loss):
        # ## YOUR CODE HERE
        optimizer = tf.train.GradientDescentOptimizer(self.config.lr)
        train_op = optimizer.minimize(loss)

        # ## END YOUR CODE
        return train_op
\end{minted}
All the basic operations in TensorFlow have attached
gradient operations. And so with the use of backpropagation TensorFlow computes the gradients for all variables in the computation graph.

\section{2}
We shall first understand \textbf{the Named Entity Recognition (NER) window model}. Suppose we have a corpus with a vocabulary $V =[\wo_{1}, \dots, \wo_{|V|}]$ (we are assuming that every word $\wo$ correspond to an index $i \in \{1, \dots, |V|\}$), a number $C$ of name entity categories (null-class,Person, Location, etc.) and a matrix $L \in \mathbb{R}^{|V|,d}$ where each row $i$ correspond to the word embedding of size $d$ of the word $\wo_{i}$. Now we can choose the parameters $m$ and $H$ to be the size window and the size of the hidden layer, respectively. Let $n = (2m+1)d$, $W \in \mathbb{R}^{n,H}$, $b_{1} \in \mathbb{R}^{H}$, $U \in \mathbb{R}^{H,C}$ and $b^{2} \in \mathbb{R}^{C}$. We assume that the training dataset is compose by training samples of the form $([\wo_{t-m}, \dots, \wo_{t}, \dots, \wo_{t+m}],c)$ where $c \in \{1, \dots, C\}$ ($1$ represent the null-class) -- this sample tell us that the word $ \wo_{t}$ is a name entity of type $c$. Let $y$ be the one-vector representation of $c$ (i.e., $y \in \mathbb{R}^{C}$ such that $y_{i} =1$ iff $i=c$), and let $x_{t-m}, \dots, x_{t}, \dots, x_{t+m}\in \mathbb{R}^{|V|}$ be the one-vector representation of $\wo_{t-m}, \dots, \wo_{t}, \dots, \wo_{t+m}$, respectively. The model is composed by the following equations:
\begin{equation}\label{eq:1}
x^{(t)} = concat([x_{t-m}L, \dots, x_{t}L, \dots, x_{t+m}L])
\end{equation}
 \begin{equation}\label{eq:2}
h = tanh(x^{(t)}W + b_{1})
\end{equation}
\begin{equation}\label{eq:3}
\hat{y} = softmax(hU + b_{2})
\end{equation}
\begin{equation}\label{eq:4}
J(W,b_{1},U,b_{2}) = CE(y,\hat{y}) = -\sum_{s=1}^{C} y_s  \log(\hat{y}_s)
\end{equation}

where $concat$ is the operation of concatenate function and $tanh$ is the hyperbolic tangent function ,i.e.,  $tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$ (we can define this function using the sigmoid function, $tanh(z) = 2sigmoid(2z)-1$). Figure \ref{ner} helps us to visualize the model.

For our particular implementation, let $C=5$ $d=50$, $m=1$, $H=100$ (hence $n=150$) and let $J(\theta)$ be an abbreviation of $J(W,b_{1},U,b_{2})$. Thus, for every training sample the loss function is
\begin{equation}\label{eq:5}
J(\theta) = CE(y,\hat{y}) = -\sum_{s=1}^{5} y_s  \log(\hat{y}_s)
\end{equation}


\begin{figure}
\begin{center}
\includegraphics[scale=0.85]{ner.pdf}
\end{center}
\caption{NER window model}
\label{ner}
\end{figure}
\textbf{2a)} Since $tanh(z) = 2\sigma(2z) -1 $ we have that, 
\begin{align*}
tanh^{\prime}(z) & = 2\sigma^{\prime}(2z)2 \\
& =4\sigma^{\prime}(2z) \\
& = 4(\sigma(2z)(1 - \sigma(2z))) \\
& = 4\sigma(2z)(4 - 4\sigma(2z))\\
& =  2(tanh(z)+1)(4 - 2(tanh(z)+1))\\
& = 2((tanh(z)+1)(2 - (tanh(z)+1)))\; .
\end{align*}
Let $\sigma^{(3)} = \hat{y} - y \in \mathbb{R}^{5}$ be the outermost error vector, hence

\begin{equation}\label{eq:6}
\frac{\partial J}{\partial  U} = {h \delta^{(3)}}^{T}
\end{equation}

\begin{equation}\label{eq:7}
\frac{\partial J}{\partial  b_{2}} =  \delta^{(3)}
\end{equation}

\begin{equation}\label{eq:8}
\delta^{(2)} = (\delta^{(3)} {(U)}^{T}) \circ {tanh^{\prime}} (h)
\end{equation}

\begin{equation}\label{eq:9}
\frac{\partial J}{\partial  W} = {x^{(t)} \delta^{(2)}}^{T}
\end{equation}

\begin{equation}\label{eq:10}
\frac{\partial J}{\partial  b_{1}} =  \delta^{(2)}
\end{equation}

First let us define $\frac{\partial J}{\partial  L_{i}}$ for the general case. Using the fact that $\frac{\partial J}{\partial  x^{(t)}}= \delta^{(2)} {(W)}^{T}$ we will define the auxiliary vectors  $v_{1}, \dots, v_{2m+1} \in \mathbb{R}^{d}$ such that for $j \in \{ 1, \dots, 2m+1\}$ 
\begin{equation}\label{eq:11}
v_{j} =  \frac{\partial J}{\partial  x^{(t)}}[(j-1)d +1: jd]
\end{equation}

Let $e$ be the enumeration function of the list $[t-m, \dots, t, \dots, t+m]$, so for $i \in \{ t-m, \dots, t, \dots, t+m \}$

\begin{equation}\label{eq:12}
\frac{\partial J}{\partial  L_{i}} =  v_{e(i)}
\end{equation}

And for $i \notin \{ t-m, \dots, t, \dots, t+m \}$ $\frac{\partial J}{\partial  L_{i}} =0$. Now for the specific case where $m=1$, 

\begin{equation}\label{eq:13}
\frac{\partial J}{\partial  L_{t-1}} =  \frac{\partial J}{\partial  x^{(t)}}[1: d]
\end{equation}
\begin{equation}\label{eq:14}
\frac{\partial J}{\partial  L_{t}} =  \frac{\partial J}{\partial  x^{(t)}}[d+1: 2d]
\end{equation}
\begin{equation}\label{eq:15}
\frac{\partial J}{\partial  L_{t+1}} =  \frac{\partial J}{\partial  x^{(t)}}[2d+1: 3d]
\end{equation}

And for $i \notin \{ t-m, \dots, t, \dots, t+m \}$ $\frac{\partial J}{\partial  L_{i}} =0$.

\textbf{2b)} To add L2 regularization to our model, we can add the following function:
\begin{equation}\label{eq:16}
J_{reg}(\theta) = \frac{\lambda}{2}[\sum_{i=1}^{n}\sum_{j=1}^{H}(W_{i,j})^{2}) + \sum_{i^{\prime}=1}^{H}\sum_{j^{\prime}=1}^{C}(U_{i^{\prime},j^{\prime}})^{2})]
\end{equation}

where $\lambda \in \mathbb{R}$ is the regularization parameter. Hence,

\begin{equation}\label{eq:17}
J_{full}(\theta) =J(\theta) + J_{reg}(\theta)
\end{equation}

The only grandients that change are in respect to $U$ and $W$. Let $\delta^{(3)}$ and $\delta^{(2)}$ be as before; then,

\begin{equation}\label{eq:18}
\frac{\partial J_{full}}{\partial  U} = {h \delta^{(3)}}^{T} + \lambda U 
\end{equation}

\begin{equation}\label{eq:19}
\frac{\partial J_{full}}{\partial  b_{2}} =  \frac{\partial J}{\partial  b_{2}}
\end{equation}

\begin{equation}\label{eq:21}
\frac{\partial J_{full}}{\partial  W} = {x^{(t)} \delta^{(2)}}^{T} + \lambda W 
\end{equation}

\begin{equation}\label{eq:22}
\frac{\partial J_{full}}{\partial  b_{1}} =  \frac{\partial J}{\partial  b_{1}} 
\end{equation}
And for $i \in \{1, \dots, |V|\}$
\begin{equation}\label{eq:23}
\frac{\partial J_{full}}{\partial  L_{i}} =  \frac{\partial J}{\partial  L_{i}} 
\end{equation}

\textbf{2c)}

\textbf{2d)}



\end{document}
